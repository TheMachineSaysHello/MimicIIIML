{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # Imports TF ops for preprocessing.\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as k\n",
    "# text preprocesssing function\n",
    "# removes redudented tokens and maps datetimes to <time_stamp>\n",
    "def _clean_text_gen(x):\n",
    "    pattern = r'.[*]+[0-9]{4}[-][0-9]+[-][0-9]+[*]+.\\s+[0-9]+[:][0-9]+\\s+[A|P]M'\n",
    "    x = re.sub(pattern, '<timestamp>', x)\n",
    "    x = re.sub('[\\n]', ' ', x)\n",
    "    tokens =[token for token in x.split(' ') if len(token)>0]\n",
    "    for token in tokens:\n",
    "        yield token\n",
    "            \n",
    "def clean_text(x):\n",
    "    results = []\n",
    "    gen =  _clean_text_gen(x)\n",
    "    try:\n",
    "        while True:\n",
    "            new_token = next(gen)\n",
    "            try:\n",
    "                if results[-1] != new_token:\n",
    "                    results.append(new_token)\n",
    "                else:\n",
    "                    pass\n",
    "            except IndexError:\n",
    "                results.append(new_token)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return ' '.join(results)\n",
    "MAX_MEMORY = \"12g\"\n",
    "data_dir = os.getenv('PHYSIO_HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Before DownSampeling:\n",
      "+----------+-------------+--------------+-----------------+------------------+------------------+\n",
      "|  CATEGORY|count(ROW_ID)|count(HADM_ID)|count(SUBJECT_ID)| avg(length(TEXT))|        avg(label)|\n",
      "+----------+-------------+--------------+-----------------+------------------+------------------+\n",
      "|Physician |       140100|          8983|             7566|5255.5330692362595|0.1745895788722341|\n",
      "+----------+-------------+--------------+-----------------+------------------+------------------+\n",
      "\n",
      "DownSampeled Data:\n",
      "(48920, 2)\n"
     ]
    }
   ],
   "source": [
    "# creates the spark context \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"dp\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "text_preprocessing = udf(lambda x :clean_text(x),StringType())\n",
    "\n",
    "\n",
    "## read in admissions data and using Hospital Expired Flag as an Outcome \n",
    "path = data_dir + '/' + 'ADMISSIONS.csv'\n",
    "admissions = spark.read.csv(path, header=True, inferSchema=True)\n",
    "admissions = admissions.withColumn('HOSPITAL_EXPIRE_FLAG', col('HOSPITAL_EXPIRE_FLAG').cast(IntegerType()))\n",
    "admissions = admissions.withColumn('HADM_ID', col('HADM_ID').cast(IntegerType()))\n",
    "\n",
    "path = data_dir + '/' + 'NOTEEVENTS.csv'\n",
    "\n",
    "# defines schema for notes\n",
    "schema = StructType([\n",
    "    StructField(\"ROW_ID\",StringType(), False),\n",
    "    StructField(\"SUBJECT_ID\",StringType(), False),\n",
    "    StructField(\"HADM_ID\",IntegerType(), True),\n",
    "    StructField(\"CHARTDATE\",DateType(), True),\n",
    "    StructField(\"CHARTTIME\",DateType(), True),\n",
    "    StructField(\"STORETIME\",DateType(), True),\n",
    "    StructField(\"CATEGORY\",StringType(), True),\n",
    "    StructField(\"DESCRIPTION\",StringType(), True),\n",
    "    StructField(\"CGID\",StringType(), True),\n",
    "    StructField(\"ISERROR\",StringType(), True),\n",
    "    StructField(\"TEXT\",StringType(), True)])\n",
    "\n",
    "# reads notes into a spark data frame\n",
    "notes = spark.read.csv(path, \n",
    "                       schema=schema, \n",
    "                       sep=',', \n",
    "                       header=True,\n",
    "                       multiLine=True,\n",
    "                        quote = '\"',\n",
    "                        escape ='\"')\\\n",
    ".filter('CATEGORY like (\"%hysic%\")')\n",
    "\n",
    "notes = notes.withColumn(\"TEXT\", text_preprocessing(\"TEXT\") )\n",
    "                         \n",
    "## Join Note to Outcomes\n",
    "notes = notes.join(admissions.select(['HADM_ID', col('HOSPITAL_EXPIRE_FLAG').alias('label')]), \n",
    "      on='HADM_ID', \n",
    "      how='inner')\n",
    "\n",
    "## Print Summary Statistics\n",
    "print('Data Before DownSampeling:')\n",
    "notes.groupby(\"CATEGORY\")\\\n",
    ".agg(countDistinct('ROW_ID'),  \n",
    "     countDistinct(\"HADM_ID\"), \n",
    "     countDistinct(\"SUBJECT_ID\"),\n",
    "     mean(length(\"TEXT\")),\n",
    "     mean(('label'))\n",
    "    )\\\n",
    ".sort(\"count(ROW_ID)\", ascending =False).show()\n",
    "\n",
    "## creates a balanced sample of positive and negative phyisicans notes\n",
    "# notes where mortality happend in the hopsital \n",
    "positive = notes\\\n",
    ".select('HADM_ID', \"TEXT\", \"label\")\\\n",
    ".filter('label == 1').toPandas()\n",
    "\n",
    "# notes where mortality did not in the hopsital, sampled in equal protortions as the the positive notes\n",
    "negative = notes\\\n",
    ".select('HADM_ID', \"TEXT\", \"label\")\\\n",
    ".filter('label == 0').limit(positive.shape[0]).toPandas()\n",
    "\n",
    "# combines negative and positive samples to create a balanced subsample\n",
    "df_balanced = pd.concat([positive, negative], axis=0).set_index('HADM_ID').sample( frac=1)\n",
    "\n",
    "print('DownSampeled Data:')\n",
    "print(df_balanced.shape)                \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39325,) (9595,)\n"
     ]
    }
   ],
   "source": [
    "group_kfold = GroupShuffleSplit(n_splits=10)\n",
    "gen = group_kfold.split(df_balanced, groups = list(df_balanced.index))\n",
    "train_index, test_index = next(gen)\n",
    "assert set( df_balanced.iloc[train_index, :].index).intersection(set( df_balanced.iloc[test_index, :].index)) == set()\n",
    "\n",
    "X_train = df_balanced.iloc[train_index, :]['TEXT'].values  \n",
    "y_train = df_balanced.iloc[train_index, :]['label'].values \n",
    "X_test = df_balanced.iloc[test_index, :]['TEXT'].values  \n",
    "y_test = df_balanced.iloc[test_index, :]['label'].values \n",
    "y = df_balanced.loc[:,'label'].values \n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# setup tensorflow datasets for training\n",
    "DATASET_SIZE = X_train.shape[0]\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "\n",
    "## Second Split for into train and eval using tensorflow pipes\n",
    "full_train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "full_train_dataset = full_train_dataset.shuffle(1)\n",
    "train_dataset = full_train_dataset.take(train_size)\n",
    "eval_dataset = full_train_dataset.skip(train_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 3442\n",
      "max_padding_len: 1555\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths_train = list(map(lambda x: len(x.split(' ')), X_train))\n",
    "max_len = np.max(sequence_lengths_train)\n",
    "max_padding_len =  int(np.mean(sequence_lengths_train) + np.std(sequence_lengths_train) * 2)\n",
    "print(F'max_len: {max_len}')\n",
    "print(F'max_padding_len: {max_padding_len}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 20000\n"
     ]
    }
   ],
   "source": [
    "from  tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "vocab_size = 20000\n",
    "print(F'vocab_size {vocab_size}')\n",
    "X_train_ds = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "preprocessor = TextVectorization(max_tokens=vocab_size,\n",
    "                                 pad_to_max_tokens=max_padding_len,\n",
    "                                 output_sequence_length=max_padding_len,\n",
    "                                output_mode='int')\n",
    "preprocessor.adapt(X_train_ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MortalityClassifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(64, 1)]                 0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 1555)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1555, 200)         4000200   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 4,000,401\n",
      "Trainable params: 4,000,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_embedding_dims = 200\n",
    "batch_size = 64\n",
    "def build_classifier_model():\n",
    "    text_input = Input(shape=(1,), batch_size=batch_size, dtype=tf.string, name='text')\n",
    "    preprocessed = preprocessor( text_input)\n",
    "    embedding  = Embedding(input_dim=vocab_size+1, \n",
    "                           output_dim=n_embedding_dims,  \n",
    "                           input_length=max_padding_len)(preprocessed)\n",
    "    pool = GlobalAveragePooling1D()(embedding)\n",
    "    drop = Dropout(0.1)(pool)\n",
    "    outputs = Dense(1, activation='sigmoid', name='classifier')(drop)\n",
    "    \n",
    "    return tf.keras.Model(text_input,outputs, name='MortalityClassifier')\n",
    "\n",
    "model = build_classifier_model()\n",
    "\n",
    "metric = tf.metrics.BinaryAccuracy(name='acc')\n",
    "loss = loss=tf.keras.losses.BinaryCrossentropy( name='loss')\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002,\n",
    "                                                             decay_steps=10000,\n",
    "                                                             decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, name='Adam')\n",
    "val_metric_name = 'val_acc'\n",
    "weights_path = 'best_weights.h5'\n",
    "\n",
    "# stops training whenmodel fails to improve\n",
    "esm =  EarlyStopping(patience=1, monitor=val_metric_name ,mode='max')\n",
    "\n",
    "# save only the best weights\n",
    "# checkpoint = ModelCheckpoint(weights_path ,\n",
    "#                              mode='max',\n",
    "#                              monitor=val_metric_name , \n",
    "#                              verbose=1, \n",
    "#                              save_best_only=True,\n",
    "#                             save_format='tf')\n",
    "# complies the model \n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "431/431 [==============================] - 75s 170ms/step - loss: 0.6828 - acc: 0.6016 - val_loss: 0.6705 - val_acc: 0.6392\n",
      "Epoch 2/20\n",
      "431/431 [==============================] - 78s 182ms/step - loss: 0.6525 - acc: 0.6567 - val_loss: 0.6338 - val_acc: 0.6807\n",
      "Epoch 3/20\n",
      "431/431 [==============================] - 77s 180ms/step - loss: 0.6106 - acc: 0.7005 - val_loss: 0.5909 - val_acc: 0.7154\n",
      "Epoch 4/20\n",
      "431/431 [==============================] - 80s 186ms/step - loss: 0.5676 - acc: 0.7339 - val_loss: 0.5511 - val_acc: 0.7431\n",
      "Epoch 5/20\n",
      "431/431 [==============================] - 74s 173ms/step - loss: 0.5280 - acc: 0.7601 - val_loss: 0.5145 - val_acc: 0.7696\n",
      "Epoch 6/20\n",
      "431/431 [==============================] - 73s 168ms/step - loss: 0.4909 - acc: 0.7853 - val_loss: 0.4800 - val_acc: 0.7935\n",
      "Epoch 7/20\n",
      "431/431 [==============================] - 73s 170ms/step - loss: 0.4558 - acc: 0.8095 - val_loss: 0.4475 - val_acc: 0.8184\n",
      "Epoch 8/20\n",
      "431/431 [==============================] - 74s 173ms/step - loss: 0.4228 - acc: 0.8329 - val_loss: 0.4176 - val_acc: 0.8369\n",
      "Epoch 9/20\n",
      "431/431 [==============================] - 72s 168ms/step - loss: 0.3926 - acc: 0.8542 - val_loss: 0.3904 - val_acc: 0.8525\n",
      "Epoch 10/20\n",
      "431/431 [==============================] - 72s 168ms/step - loss: 0.3655 - acc: 0.8692 - val_loss: 0.3659 - val_acc: 0.8667\n",
      "Epoch 11/20\n",
      "431/431 [==============================] - 73s 170ms/step - loss: 0.3411 - acc: 0.8822 - val_loss: 0.3439 - val_acc: 0.8774\n",
      "Epoch 12/20\n",
      "431/431 [==============================] - 72s 168ms/step - loss: 0.3184 - acc: 0.8941 - val_loss: 0.3240 - val_acc: 0.8885\n",
      "Epoch 13/20\n",
      "431/431 [==============================] - 73s 169ms/step - loss: 0.2983 - acc: 0.9043 - val_loss: 0.3061 - val_acc: 0.8978\n",
      "Epoch 14/20\n",
      "431/431 [==============================] - 74s 172ms/step - loss: 0.2802 - acc: 0.9113 - val_loss: 0.2899 - val_acc: 0.9057\n",
      "Epoch 15/20\n",
      "431/431 [==============================] - 75s 175ms/step - loss: 0.2636 - acc: 0.9178 - val_loss: 0.2751 - val_acc: 0.9114\n",
      "Epoch 16/20\n",
      "431/431 [==============================] - 74s 173ms/step - loss: 0.2485 - acc: 0.9237 - val_loss: 0.2617 - val_acc: 0.9150\n",
      "Epoch 17/20\n",
      "431/431 [==============================] - 74s 172ms/step - loss: 0.2344 - acc: 0.9290 - val_loss: 0.2494 - val_acc: 0.9193\n",
      "Epoch 18/20\n",
      "431/431 [==============================] - 82s 189ms/step - loss: 0.2216 - acc: 0.9334 - val_loss: 0.2382 - val_acc: 0.9209\n",
      "Epoch 19/20\n",
      "431/431 [==============================] - 78s 181ms/step - loss: 0.2100 - acc: 0.9365 - val_loss: 0.2279 - val_acc: 0.9239\n",
      "Epoch 20/20\n",
      "431/431 [==============================] - 72s 166ms/step - loss: 0.1992 - acc: 0.9402 - val_loss: 0.2184 - val_acc: 0.9272\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "n_epochs = 20\n",
    "history = model.fit(train_dataset.batch(batch_size), \n",
    "                      validation_data=eval_dataset.batch(batch_size), \n",
    "                      shuffle=True,\n",
    "                       callbacks=[esm],\n",
    "                      epochs=n_epochs  \n",
    "                   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "# predicts test set\n",
    "test_preds = model.predict(test_dataset.batch(batch_size)).flatten()\n",
    "test_pred_labels = np.array([1 if v > threshold else 0 for v in test_preds])\n",
    "\n",
    "# predicts training set\n",
    "train_preds = model.predict(full_train_dataset.batch(batch_size)).flatten()\n",
    "train_pred_labels = np.array([1 if v > threshold else 0 for v in train_preds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.869221294512772 on test\n",
      "roc_auc_score: 0.984839800754114 on train\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "# calcuates metrics on test data\n",
    "test_f1 = f1_score(y_test, test_pred_labels)\n",
    "test_acc_balanced = balanced_accuracy_score(y_test, test_pred_labels)\n",
    "test_acc = accuracy_score(y_test, test_pred_labels)\n",
    "test_precision = precision_score(y_test, test_pred_labels)\n",
    "test_recall = recall_score(y_test, test_pred_labels)\n",
    "test_auc_score = roc_auc_score(y[test_index], test_preds)\n",
    "print(F'roc_auc_score: {test_auc_score } on test')\n",
    "\n",
    "# calculates metrics on training data \n",
    "train_f1 = f1_score(y_train, train_pred_labels)\n",
    "train_acc_balanced = balanced_accuracy_score(y_train, train_pred_labels)\n",
    "train_acc = accuracy_score(y_train, train_pred_labels)\n",
    "train_precision = precision_score(y_train, train_pred_labels)\n",
    "train_recall = recall_score(y_train, train_pred_labels)\n",
    "train_auc_score = roc_auc_score(y_train, train_preds)\n",
    "print(F'roc_auc_score: {train_auc_score} on train')\n",
    "\n",
    "# gets params Artifacts for logging mlflow model\n",
    "n_cases = np.sum(y == 1)\n",
    "n_controls = np.sum(y == 0)\n",
    "n_train_obs = X_train.shape[0]\n",
    "n_test_obs = X_test.shape[0]\n",
    "\n",
    "train_label_prob = y_train.mean()\n",
    "test_label_prob = y_test.mean()\n",
    "desc = str(model.to_json())\n",
    "model_type = type(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MortalityClassifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(64, 1)]                 0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 1555)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1555, 200)         4000200   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 4,000,401\n",
      "Trainable params: 4,000,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "input_example = X_train[1:10]\n",
    "signature = infer_signature(input_example, model.predict(input_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking uri: http://localhost:5000\n",
      "Artifact uri: ./mlruns/1/18a7c18146f04d908fb0d9435a56b7c9/artifacts\n",
      "logging experiment_id: \"1\" run_id :\"18a7c18146f04d908fb0d9435a56b7c9\" completed\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "artifact_path = 'Model'\n",
    "data_grain = 'NOTE_ID'\n",
    "label_name = 'HOSPITAL_EXPIRE_FLAG'\n",
    "data_source = 'PhysioMimicIII'\n",
    "run_name = 'CNN_Embedding_wo_initialization'\n",
    "experiment_id = 1\n",
    "tracking_uri = \"http://localhost:5000\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "experiment_id=1\n",
    "with mlflow.start_run(run_name=run_name, experiment_id=experiment_id) as run:\n",
    "    \n",
    "    tracking_uri = mlflow.get_tracking_uri()\n",
    "    artifact_uri = mlflow.get_artifact_uri()\n",
    "    \n",
    "    print(\"Tracking uri: {}\".format(tracking_uri))\n",
    "    print(\"Artifact uri: {}\".format(artifact_uri))\n",
    "    mlflow.log_metric('n_epochs', len(history.history['loss']))\n",
    "    for key in list(history.history.keys()):   \n",
    "        for i, j in enumerate(history.history[key]):\n",
    "            mlflow.log_metric(key, j, step=i)\n",
    "    mlflow.log_param('data_source', data_source)\n",
    "    mlflow.log_param('label_name', label_name)\n",
    "    mlflow.log_param('data_grain', data_grain)\n",
    "    mlflow.log_param('n_cases', n_cases)\n",
    "    mlflow.log_param('n_controls', n_controls)\n",
    "    mlflow.log_param('n_train_obs', n_train_obs)\n",
    "    mlflow.log_param('n_test_obs', n_test_obs)\n",
    "    mlflow.log_param('train_label_prob', train_label_prob)\n",
    "    mlflow.log_param('test_label_prob', test_label_prob)\n",
    "    mlflow.log_param('desc', desc)\n",
    "    mlflow.log_param('model_type',model_type)\n",
    "    mlflow.log_metric('train_f1', train_f1)\n",
    "    mlflow.log_metric('train_acc_balanced', train_acc_balanced)\n",
    "    mlflow.log_metric('train_acc', train_acc)\n",
    "    mlflow.log_metric('train_precision', train_precision)\n",
    "    mlflow.log_metric('train_recall', train_recall)\n",
    "    mlflow.log_metric('train_auc_score', train_auc_score)\n",
    "    mlflow.log_metric('test_f1', test_f1)\n",
    "    mlflow.log_metric('test_acc_balanced', test_acc_balanced)\n",
    "    mlflow.log_metric('test_acc', test_acc)\n",
    "    mlflow.log_metric('test_precision', test_precision)\n",
    "    mlflow.log_metric('test_recall', test_recall)\n",
    "    mlflow.log_metric('test_auc_score', test_auc_score)\n",
    "    run_id = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id \n",
    "    mlflow.end_run()\n",
    "    print(F'logging experiment_id: \"{experiment_id}\" run_id :\"{run_id}\" completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
